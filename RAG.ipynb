{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SimpleRAG:\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 llm_model=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "        \"\"\"\n",
        "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã\n",
        "\n",
        "        ‚ö†Ô∏è –ß–¢–û –ú–ï–ù–Ø–¢–¨ –ó–î–ï–°–¨:\n",
        "        - model_name: –Ω–∞ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"sentence-transformers/all-mpnet-base-v2\")\n",
        "        - llm_model: –Ω–∞ –¥—Ä—É–≥–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"google/flan-t5-large\", \"facebook/bart-large\")\n",
        "        - device_map: –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ \"cuda\" –µ—Å–ª–∏ –µ—Å—Ç—å GPU, –∏–ª–∏ \"cpu\" –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è CPU\n",
        "        \"\"\"\n",
        "        # –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
        "        self.embedding_model = SentenceTransformer(model_name)\n",
        "\n",
        "        # ‚ö†Ô∏è –ï–°–õ–ò –ù–ï–¢ GPU –ò–õ–ò –ú–ê–õ–û –ü–ê–ú–Ø–¢–ò:\n",
        "        # self.embedding_model = SentenceTransformer(model_name, device='cpu')\n",
        "\n",
        "        # –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "        # ‚ö†Ô∏è –î–õ–Ø –£–ü–†–û–©–ï–ù–ò–Ø –ò–õ–ò –ï–°–õ–ò –ù–ï–¢ –î–û–°–¢–£–ü–ê –ö –ë–û–õ–¨–®–ò–ú –ú–û–î–ï–õ–Ø–ú:\n",
        "        # –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–∏–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –≤–æ–æ–±—â–µ —É–±—Ä–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é\n",
        "        try:\n",
        "            self.generator = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=llm_model,\n",
        "                torch_dtype=torch.float16,  # ‚ö†Ô∏è –ú–ï–ù–Ø–¢–¨ –ù–ê torch.float32 –µ—Å–ª–∏ –Ω–µ—Ç GPU\n",
        "                device_map=\"auto\",          # ‚ö†Ô∏è –ú–ï–ù–Ø–¢–¨ –ù–ê \"cpu\" –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å GPU\n",
        "                max_length=512\n",
        "            )\n",
        "            self.use_generation = True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å: {e}\")\n",
        "            print(\"‚ö†Ô∏è –ë—É–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\")\n",
        "            self.use_generation = False\n",
        "\n",
        "        # FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
        "        self.index = None\n",
        "        self.documents = []\n",
        "\n",
        "    def load_documents(self, documents: List[str]):\n",
        "        \"\"\"\n",
        "        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–∏—Å—Ç–µ–º—É\n",
        "\n",
        "        ‚ö†Ô∏è –ß–¢–û –ú–ï–ù–Ø–¢–¨ –ó–î–ï–°–¨:\n",
        "        - –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ (–æ—á–∏—Å—Ç–∫–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\n",
        "        - –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
        "        - –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —á–∞–Ω–∫–∏–Ω–≥ (—Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)\n",
        "        \"\"\"\n",
        "        # ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–£:\n",
        "        # documents = [self.preprocess_text(doc) for doc in documents]\n",
        "\n",
        "        # ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –ß–ê–ù–ö–ò–ù–ì –î–õ–Ø –î–õ–ò–ù–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í:\n",
        "        # chunked_docs = []\n",
        "        # for doc in documents:\n",
        "        #     chunks = self.chunk_text(doc, chunk_size=500)\n",
        "        #     chunked_docs.extend(chunks)\n",
        "        # self.documents = chunked_docs\n",
        "\n",
        "        self.documents = documents\n",
        "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–£ –¢–ï–ö–°–¢–ê:\n",
        "        - –û—á–∏—Å—Ç–∫–∞ –æ—Ç HTML —Ç–µ–≥–æ–≤\n",
        "        - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "        - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "        - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ —Ç.–¥.\n",
        "        \"\"\"\n",
        "        # –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏:\n",
        "        import re\n",
        "        text = re.sub(r'\\s+', ' ', text)  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
        "        text = text.strip()\n",
        "        # text = text.lower()  # ‚ö†Ô∏è –†–ê–°–ö–û–ú–ú–ï–ù–¢–ò–†–û–í–ê–¢–¨ –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä\n",
        "        return text\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        \"\"\"\n",
        "        ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –†–ê–ó–ë–ò–ï–ù–ò–ï –¢–ï–ö–°–¢–ê –ù–ê –ß–ê–ù–ö–ò:\n",
        "        - –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "        - chunk_size: —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö/—Å–ª–æ–≤–∞—Ö\n",
        "        - overlap: –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏\n",
        "        \"\"\"\n",
        "        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞ –ø–æ —Å–ª–æ–≤–∞–º\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk = ' '.join(words[i:i + chunk_size])\n",
        "            chunks.append(chunk)\n",
        "            if i + chunk_size >= len(words):\n",
        "                break\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞\"\"\"\n",
        "        if not self.documents:\n",
        "            raise ValueError(\"–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "        print(\"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\")\n",
        "        embeddings = self.embedding_model.encode(self.documents)\n",
        "\n",
        "        # ‚ö†Ô∏è –í–ê–†–ò–ê–ù–¢–´ –ò–ù–î–ï–ö–°–û–í FAISS (–≤—ã–±–∏—Ä–∞—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö):\n",
        "        # IndexFlatIP - –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö datasets (—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫)\n",
        "        # IndexIVFFlat - –¥–ª—è –±–æ–ª—å—à–∏—Ö datasets (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫, –±—ã—Å—Ç—Ä–µ–µ)\n",
        "        dimension = embeddings.shape[1]\n",
        "\n",
        "        if len(self.documents) < 10000:\n",
        "            # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π\n",
        "            self.index = faiss.IndexFlatIP(dimension)\n",
        "        else:\n",
        "            # ‚ö†Ô∏è –î–õ–Ø –ë–û–õ–¨–®–ò–• –ö–û–õ–õ–ï–ö–¶–ò–ô (>10–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):\n",
        "            quantizer = faiss.IndexFlatIP(dimension)\n",
        "            nlist = 100  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤\n",
        "            self.index = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
        "            # –Ω—É–∂–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å\n",
        "            self.index.train(embeddings)\n",
        "\n",
        "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "        print(f\"–ü–æ—Å—Ç—Ä–æ–µ–Ω –∏–Ω–¥–µ–∫—Å —Å {self.index.ntotal} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏\")\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3, score_threshold: float = 0.0) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "\n",
        "        ‚ö†Ô∏è –ß–¢–û –ú–ï–ù–Ø–¢–¨ –ó–î–ï–°–¨:\n",
        "        - k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "        - score_threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
        "        - –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞\n",
        "        \"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –∏–Ω–¥–µ–∫—Å\")\n",
        "\n",
        "        # ‚ö†Ô∏è –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–ü–†–û–°–ê:\n",
        "        # query = self.preprocess_text(query)\n",
        "\n",
        "        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ\n",
        "        scores, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –æ—Ü–µ–Ω–∫–∏\n",
        "        results = []\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            if idx < len(self.documents) and score >= score_threshold:\n",
        "                results.append((self.documents[idx], float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_answer(self, query: str, context: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
        "\n",
        "        ‚ö†Ô∏è –ß–¢–û –ú–ï–ù–Ø–¢–¨ –ó–î–ï–°–¨:\n",
        "        - –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ (prompt template)\n",
        "        - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (temperature, max_tokens)\n",
        "        - –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
        "        \"\"\"\n",
        "        if not self.use_generation:\n",
        "            # ‚ö†Ô∏è –ï–°–õ–ò –ì–ï–ù–ï–†–ê–¶–ò–Ø –ù–ï–î–û–°–¢–£–ü–ù–ê - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π –¥–æ–∫—É–º–µ–Ω—Ç\n",
        "            return f\"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {context[0][:200]}...\" if context else \"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
        "\n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
        "        context_text = \"\\n\".join([f\"- {doc}\" for doc in context])\n",
        "\n",
        "        # ‚ö†Ô∏è –®–ê–ë–õ–û–ù –ü–†–û–ú–ü–¢–ê - –ú–û–ñ–ù–û –ò–ó–ú–ï–ù–ò–¢–¨ –ü–û–î –ó–ê–î–ê–ß–£:\n",
        "        prompt = f\"\"\"–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.\n",
        "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.\n",
        "\n",
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n",
        "{context_text}\n",
        "\n",
        "–í–æ–ø—Ä–æ—Å: {query}\n",
        "\n",
        "–û—Ç–≤–µ—Ç: \"\"\"\n",
        "\n",
        "        # ‚ö†Ô∏è –ü–ê–†–ê–ú–ï–¢–†–´ –ì–ï–ù–ï–†–ê–¶–ò–ò - –ú–û–ñ–ù–û –ù–ê–°–¢–†–û–ò–¢–¨:\n",
        "        try:\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_new_tokens=256,      # ‚ö†Ô∏è –ú–ï–ù–Ø–¢–¨: –±–æ–ª—å—à–µ/–º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤\n",
        "                do_sample=True,          # ‚ö†Ô∏è –ú–ï–ù–Ø–¢–¨: False –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞\n",
        "                temperature=0.7,         # ‚ö†Ô∏è –ú–ï–ù–Ø–¢–¨: 0.1-1.0 (–º–µ–Ω—å—à–µ = –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–æ)\n",
        "                top_p=0.9,               # ‚ö†Ô∏è –ú–ï–ù–Ø–¢–¨: 0.5-1.0 (nucleus sampling)\n",
        "                pad_token_id=self.generator.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            generated_text = response[0]['generated_text']\n",
        "            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–ø–æ—Å–ª–µ \"–û—Ç–≤–µ—Ç: \")\n",
        "            if \"–û—Ç–≤–µ—Ç: \" in generated_text:\n",
        "                return generated_text.split(\"–û—Ç–≤–µ—Ç: \")[-1].strip()\n",
        "            else:\n",
        "                return generated_text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}\")\n",
        "            return f\"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {context[0][:300]}...\" if context else \"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç\"\n",
        "\n",
        "    def ask(self, query: str, k: int = 3, include_sources: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω RAG: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "\n",
        "        ‚ö†Ô∏è –ß–¢–û –ú–ï–ù–Ø–¢–¨ –ó–î–ï–°–¨:\n",
        "        - k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
        "        - include_sources: –≤–∫–ª—é—á–∞—Ç—å –ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–≤–µ—Ç\n",
        "        - –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –∏—Å—Ç–æ—á–Ω–∏–∫—É –∏ —Ç.–¥.\n",
        "        \"\"\"\n",
        "        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "        retrieved_docs = self.retrieve(query, k)\n",
        "\n",
        "        if not retrieved_docs:\n",
        "            return {\n",
        "                \"answer\": \"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\",\n",
        "                \"sources\": [],\n",
        "                \"query\": query\n",
        "            }\n",
        "\n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "        context_texts = [doc for doc, score in retrieved_docs]\n",
        "\n",
        "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\n",
        "        answer = self.generate_answer(query, context_texts)\n",
        "\n",
        "        result = {\n",
        "            \"answer\": answer,\n",
        "            \"query\": query\n",
        "        }\n",
        "\n",
        "        if include_sources:\n",
        "            result[\"sources\"] = retrieved_docs\n",
        "\n",
        "        return result\n",
        "\n",
        "# ‚ö†Ô∏è –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –†–ê–ó–ù–´–• –¢–ò–ü–û–í –î–ê–ù–ù–´–•\n",
        "def load_data_from_source(source_type: str, source_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
        "\n",
        "    ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –ü–û–î–î–ï–†–ñ–ö–£ –ù–£–ñ–ù–´–• –§–û–†–ú–ê–¢–û–í:\n",
        "    - txt, csv, json, pdf, docx, –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ç.–¥.\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "\n",
        "    if source_type == \"txt\":\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞\n",
        "        try:\n",
        "            with open(source_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                # ‚ö†Ô∏è –ù–ê–°–¢–†–û–ò–¢–¨ –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–û–í:\n",
        "                documents = [doc.strip() for doc in content.split('\\n\\n') if doc.strip()]\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {source_path}: {e}\")\n",
        "\n",
        "    elif source_type == \"csv\":\n",
        "        # ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –ó–ê–ì–†–£–ó–ö–£ –ò–ó CSV:\n",
        "        try:\n",
        "            df = pd.read_csv(source_path)\n",
        "            # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º\n",
        "            if 'text' in df.columns:\n",
        "                documents = df['text'].dropna().tolist()\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV {source_path}: {e}\")\n",
        "\n",
        "    elif source_type == \"json\":\n",
        "        # ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –ó–ê–ì–†–£–ó–ö–£ –ò–ó JSON:\n",
        "        try:\n",
        "            with open(source_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON\n",
        "                if isinstance(data, list):\n",
        "                    documents = [str(item) for item in data]\n",
        "        except Exception as e:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ JSON {source_path}: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_type}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "# ‚ö†Ô∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø - –ú–ï–ù–Ø–¢–¨ –ü–ê–†–ê–ú–ï–¢–†–´ –ó–î–ï–°–¨\n",
        "class RAGConfig:\n",
        "    \"\"\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã\"\"\"\n",
        "\n",
        "    # ‚ö†Ô∏è –ú–û–î–ï–õ–ò\n",
        "    EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: \"all-mpnet-base-v2\"\n",
        "    LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"           # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã: \"facebook/bart-large\"\n",
        "\n",
        "    # ‚ö†Ô∏è –ü–ê–†–ê–ú–ï–¢–†–´ –ü–û–ò–°–ö–ê\n",
        "    TOP_K = 3                   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "    SCORE_THRESHOLD = 0.0       # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
        "\n",
        "    # ‚ö†Ô∏è –ü–ê–†–ê–ú–ï–¢–†–´ –ì–ï–ù–ï–†–ê–¶–ò–ò\n",
        "    MAX_NEW_TOKENS = 256\n",
        "    TEMPERATURE = 0.7\n",
        "    DO_SAMPLE = True\n",
        "\n",
        "    # ‚ö†Ô∏è –ü–ê–†–ê–ú–ï–¢–†–´ –î–ê–ù–ù–´–•\n",
        "    CHUNK_SIZE = 500            # –î–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞\n",
        "    CHUNK_OVERLAP = 50          # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    ‚ö†Ô∏è –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø - –ú–ï–ù–Ø–¢–¨ –ü–û–î –°–í–û–ò –î–ê–ù–ù–´–ï –ò –ó–ê–î–ê–ß–ò\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. ‚ö†Ô∏è –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –° –ö–û–ù–§–ò–ì–û–ú\n",
        "    rag_system = SimpleRAG(\n",
        "        model_name=RAGConfig.EMBEDDING_MODEL,\n",
        "        llm_model=RAGConfig.LLM_MODEL\n",
        "    )\n",
        "\n",
        "    # 2. ‚ö†Ô∏è –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• - –í–´–ë–†–ê–¢–¨ –ù–£–ñ–ù–´–ô –°–ü–û–°–û–ë:\n",
        "\n",
        "    # –°–ø–æ—Å–æ–± A: –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤\n",
        "    documents = [\n",
        "        \"–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫...\",\n",
        "        \"–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞...\",\n",
        "        # ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –°–í–û–ò –î–û–ö–£–ú–ï–ù–¢–´ –ó–î–ï–°–¨\n",
        "    ]\n",
        "\n",
        "    # –°–ø–æ—Å–æ–± B: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞\n",
        "    # documents = load_data_from_source(\"txt\", \"data/my_documents.txt\")\n",
        "    # documents = load_data_from_source(\"csv\", \"data/documents.csv\")\n",
        "    # documents = load_data_from_source(\"json\", \"data/documents.json\")\n",
        "\n",
        "    # –°–ø–æ—Å–æ–± C: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ API\n",
        "    # documents = load_from_database()  # ‚ö†Ô∏è –†–ï–ê–õ–ò–ó–û–í–ê–¢–¨ –§–£–ù–ö–¶–ò–Æ\n",
        "\n",
        "    rag_system.load_documents(documents)\n",
        "    rag_system.build_index()\n",
        "\n",
        "    # 3. ‚ö†Ô∏è –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï - –î–û–ë–ê–í–ò–¢–¨ –°–í–û–ò –ó–ê–ü–†–û–°–´\n",
        "    test_queries = [\n",
        "        \"–ß—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?\",\n",
        "        \"–ö–∞–∫–∏–µ –µ—Å—Ç—å –≤–∏–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è?\",\n",
        "        # ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ –°–í–û–ò –¢–ï–°–¢–û–í–´–ï –ó–ê–ü–†–û–°–´\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï RAG –°–ò–°–¢–ï–ú–´\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nüìù –í–æ–ø—Ä–æ—Å: {query}\")\n",
        "        result = rag_system.ask(query, k=RAGConfig.TOP_K)\n",
        "\n",
        "        print(f\"ü§ñ –û—Ç–≤–µ—Ç: {result['answer']}\")\n",
        "        if 'sources' in result:\n",
        "            print(\"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:\")\n",
        "            for i, (source, score) in enumerate(result['sources']):\n",
        "                print(f\"   {i+1}. [—Å—Ö–æ–¥—Å—Ç–≤–æ: {score:.3f}] {source[:100]}...\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    # 4. ‚ö†Ô∏è –ì–ï–ù–ï–†–ê–¶–ò–Ø –°–ê–ë–ú–ò–¢–ê - –ù–ê–°–¢–†–û–ò–¢–¨ –ü–û–î –§–û–†–ú–ê–¢ –¢–†–ï–ë–û–í–ê–ù–ò–ô\n",
        "    generate_submission(\n",
        "        test_queries,\n",
        "        rag_system,\n",
        "        output_file=\"my_rag_predictions.json\"\n",
        "    )\n",
        "\n",
        "def generate_submission(test_queries: List[str], rag_system: SimpleRAG,\n",
        "                       output_file: str = \"rag_predictions.json\"):\n",
        "    \"\"\"\n",
        "    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–±–º–∏—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤\n",
        "\n",
        "    ‚ö†Ô∏è –ù–ê–°–¢–†–û–ò–¢–¨ –§–û–†–ú–ê–¢ –í–´–í–û–î–ê –ü–û–î –¢–†–ï–ë–û–í–ê–ù–ò–Ø:\n",
        "    - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON\n",
        "    - –í–∫–ª—é—á–∞–µ–º—ã–µ –ø–æ–ª—è\n",
        "    - –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for i, query in enumerate(test_queries):\n",
        "        print(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {i+1}/{len(test_queries)}: {query}\")\n",
        "\n",
        "        result = rag_system.ask(query, k=RAGConfig.TOP_K)\n",
        "\n",
        "        # ‚ö†Ô∏è –ù–ê–°–¢–†–û–ò–¢–¨ –°–¢–†–£–ö–¢–£–†–£ –°–ê–ë–ú–ò–¢–ê:\n",
        "        prediction = {\n",
        "            \"id\": i + 1,                    # ‚ö†Ô∏è –ú–û–ñ–ï–¢ –ë–´–¢–¨ –î–†–£–ì–û–ô ID\n",
        "            \"query\": query,\n",
        "            \"answer\": result[\"answer\"],\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"text\": doc[:200] + \"...\",  # ‚ö†Ô∏è –ù–ê–°–¢–†–û–ò–¢–¨ –î–õ–ò–ù–£ –¢–ï–ö–°–¢–ê\n",
        "                    \"score\": float(score),\n",
        "                    \"source_id\": j            # ‚ö†Ô∏è –î–û–ë–ê–í–ò–¢–¨ ID –ò–°–¢–û–ß–ù–ò–ö–ê –ï–°–õ–ò –ï–°–¢–¨\n",
        "                }\n",
        "                for j, (doc, score) in enumerate(result.get(\"sources\", []))\n",
        "            ],\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat()  # ‚ö†Ô∏è –û–ü–¶–ò–û–ù–ê–õ–¨–ù–û\n",
        "        }\n",
        "\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    # ‚ö†Ô∏è –ù–ê–°–¢–†–û–ò–¢–¨ –§–û–†–ú–ê–¢ –°–û–•–†–ê–ù–ï–ù–ò–Ø:\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ –°–∞–±–º–∏—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_file}\")\n",
        "    print(f\"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(predictions)}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ‚ö†Ô∏è –í–ö–õ–Æ–ß–ò–¢–¨/–í–´–ö–õ–Æ–ß–ò–¢–¨ –†–ê–ó–ù–´–ï –ß–ê–°–¢–ò:\n",
        "\n",
        "    # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "    main()\n",
        "\n",
        "    # –ò–ª–∏ —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "    # test_my_rag()\n",
        "\n",
        "    # –ò–ª–∏ —Ç–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∞–±–º–∏—Ç–∞\n",
        "    # generate_my_submission()"
      ],
      "metadata": {
        "id": "rVwFU0wYBe1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}